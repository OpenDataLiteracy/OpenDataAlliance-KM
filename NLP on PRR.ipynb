{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Techniques on Public Records Requests Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "#from sklearn.manifold import TSNE\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from gensim.models import Word2Vec, ldamodel\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re\n",
    "import time\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "# this isn't strictly an import, but it's used globally\n",
    "# so it's up here. change the model for different word vectors\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and structuring data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the onset of this project, we obtained data extracts from a number of institutions and municipalities of different sizes, with the intention of collating them into one repository for analysis. Upon receipt, there were a handful of things that we realized, which dramatically reduced the kind of analysis that we could perform on the data.\n",
    "\n",
    "1. **Government bodies have different reporting standards for metadata.** I believe this is related to the platform/infrastructure the institution is using to fulfill and manage requests. The fields of interest, as well as the types of data accessible within those fields, are often so distinct that they cannot be combined in a straightforward way. For example, some municipalities have tons of great metadata reporting, especially around the estimated amount of time and actual amount of time that it took to fulfill a request.\n",
    "\n",
    "2. **Inputs are not standardized.** Both people and institutions submit public records requests, which means that there are differing degrees of detail, complexity, and formatting. Some people submit a few words or a case number in their request, while others copy-paste several spreadsheet columns. \n",
    "\n",
    "3. **There is personal identifying information everywhere.** The more I dug into records requests, the more I saw people signing their requests with their name, address, and contact information. It's hard to strip out this information with software, especially because requests ask for information on a particular person or named entity. This is problematic for open data projects, namely because \n",
    "\n",
    "4. **All data are equal, but some data are more equal than others.** This project charts out some methods that can be used across departments that have different numbers of records requests, but there's little we can do (and little inference we can provide) with a solitary request.\n",
    "\n",
    "The confluence of all of these factors effectively means that it's hard to look for trends. *Not great.*\n",
    "\n",
    "It's really important to clean and harmonize the data as best we can. As such, we ended up throwing away most of the data that we had and working with a much smaller subset. After importing the data into a pandas dataframe, we kept:\n",
    "\n",
    "- The department name (a string);\n",
    "- Record creation date (a datetime object)\n",
    "- Request summary (a string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this section loads an excel sheet.\n",
    "# it will be updated to connect to a database in the future.\n",
    "#\n",
    "data = pd.read_excel('reformatted.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23399"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reindex the data above, using only the columns we care about\n",
    "truncated = data[['department_name', 'create_date', 'request_summary']]\n",
    "\n",
    "# then drop all of the null values\n",
    "truncated = truncated.dropna()\n",
    "\n",
    "# number of entries in the truncated data block\n",
    "len(truncated)\n",
    "\n",
    "#sum(truncated.request_type_description != truncated.spd_overall_rec_req_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department_name</th>\n",
       "      <th>pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPD</th>\n",
       "      <td>13789</td>\n",
       "      <td>0.58930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SFD</th>\n",
       "      <td>3976</td>\n",
       "      <td>0.16992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Site Administrator</th>\n",
       "      <td>939</td>\n",
       "      <td>0.04013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCI</th>\n",
       "      <td>809</td>\n",
       "      <td>0.03457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAS</th>\n",
       "      <td>740</td>\n",
       "      <td>0.03163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOT</th>\n",
       "      <td>667</td>\n",
       "      <td>0.02851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAW</th>\n",
       "      <td>297</td>\n",
       "      <td>0.01269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LEG</th>\n",
       "      <td>294</td>\n",
       "      <td>0.01256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCL</th>\n",
       "      <td>279</td>\n",
       "      <td>0.01192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPU</th>\n",
       "      <td>270</td>\n",
       "      <td>0.01154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOS</th>\n",
       "      <td>224</td>\n",
       "      <td>0.00957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PKS</th>\n",
       "      <td>196</td>\n",
       "      <td>0.00838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OCR</th>\n",
       "      <td>151</td>\n",
       "      <td>0.00645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHR</th>\n",
       "      <td>119</td>\n",
       "      <td>0.00509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HSD</th>\n",
       "      <td>89</td>\n",
       "      <td>0.00380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RET</th>\n",
       "      <td>76</td>\n",
       "      <td>0.00325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCD</th>\n",
       "      <td>69</td>\n",
       "      <td>0.00295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ITD</th>\n",
       "      <td>65</td>\n",
       "      <td>0.00278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DON</th>\n",
       "      <td>58</td>\n",
       "      <td>0.00248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EEC</th>\n",
       "      <td>44</td>\n",
       "      <td>0.00188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OED</th>\n",
       "      <td>37</td>\n",
       "      <td>0.00158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CEN</th>\n",
       "      <td>36</td>\n",
       "      <td>0.00154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFH</th>\n",
       "      <td>33</td>\n",
       "      <td>0.00141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>25</td>\n",
       "      <td>0.00107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOE</th>\n",
       "      <td>17</td>\n",
       "      <td>0.00073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSE</th>\n",
       "      <td>17</td>\n",
       "      <td>0.00073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBO</th>\n",
       "      <td>12</td>\n",
       "      <td>0.00051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OIR</th>\n",
       "      <td>11</td>\n",
       "      <td>0.00047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPL</th>\n",
       "      <td>11</td>\n",
       "      <td>0.00047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HXM</th>\n",
       "      <td>9</td>\n",
       "      <td>0.00038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ART</th>\n",
       "      <td>8</td>\n",
       "      <td>0.00034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CIV</th>\n",
       "      <td>8</td>\n",
       "      <td>0.00034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZZZ</th>\n",
       "      <td>7</td>\n",
       "      <td>0.00030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMR</th>\n",
       "      <td>7</td>\n",
       "      <td>0.00030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPC</th>\n",
       "      <td>5</td>\n",
       "      <td>0.00021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMC</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPN</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    department_name      pct\n",
       "SPD                           13789  0.58930\n",
       "SFD                            3976  0.16992\n",
       "Site Administrator              939  0.04013\n",
       "SCI                             809  0.03457\n",
       "FAS                             740  0.03163\n",
       "DOT                             667  0.02851\n",
       "LAW                             297  0.01269\n",
       "LEG                             294  0.01256\n",
       "SCL                             279  0.01192\n",
       "SPU                             270  0.01154\n",
       "MOS                             224  0.00957\n",
       "PKS                             196  0.00838\n",
       "OCR                             151  0.00645\n",
       "SHR                             119  0.00509\n",
       "HSD                              89  0.00380\n",
       "RET                              76  0.00325\n",
       "PCD                              69  0.00295\n",
       "ITD                              65  0.00278\n",
       "DON                              58  0.00248\n",
       "EEC                              44  0.00188\n",
       "OED                              37  0.00158\n",
       "CEN                              36  0.00154\n",
       "OFH                              33  0.00141\n",
       "OLS                              25  0.00107\n",
       "DOE                              17  0.00073\n",
       "OSE                              17  0.00073\n",
       "CBO                              12  0.00051\n",
       "OIR                              11  0.00047\n",
       "SPL                              11  0.00047\n",
       "HXM                               9  0.00038\n",
       "ART                               8  0.00034\n",
       "CIV                               8  0.00034\n",
       "ZZZ                               7  0.00030\n",
       "IMR                               7  0.00030\n",
       "CPC                               5  0.00021\n",
       "SMC                               4  0.00017\n",
       "PPN                               1  0.00004"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's a list of all of the possible departments we can choose from\n",
    "# \n",
    "depts = truncated.department_name.value_counts()\n",
    "deptsdf = pd.DataFrame(depts)\n",
    "deptsdf['pct'] = (deptsdf['department_name'] / len(truncated)).round(5)\n",
    "deptsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# departments to handle differently because they have a large number of req's\n",
    "# SPD; FAS; SFD; HSD; DOT; LAW\n",
    "spd = truncated[truncated.department_name != 'SPD']\n",
    "zzz = truncated[truncated.department_name == 'ZZZ']\n",
    "#truncated.groupby(['department_name', 'request_type_description']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 0: Helper functions and generating a word cloud for each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences(df, col):\n",
    "    # ARGUMENTS: df; a pandas dataframe\n",
    "    # col, a string; a column of a particular pandas dataframe\n",
    "    #\n",
    "    sents = []\n",
    "    #\n",
    "    # this function takes information out of a table\n",
    "    # and dumps it into a list for further processing\n",
    "    for row in df[col]:\n",
    "        sents.append(str(row))\n",
    "    return sents\n",
    "\n",
    "def nlp_proc(data):\n",
    "    # ARGUMENTS: data, either a string or a list\n",
    "    # OUTPUTS: spacy processed text data\n",
    "    #\n",
    "    # this checks to see if the data we are processing is a list\n",
    "    # and if it is, runs the NLP function on the different huge\n",
    "    # strings in the list, returning the list\n",
    "    if type(data) == list:\n",
    "        nlplist = [ nlp(i) for i in data ]\n",
    "        return nlplist\n",
    "    else:\n",
    "        # otherwise it just processes the data; just a backup\n",
    "        return nlp(data)\n",
    "    \n",
    "def filter_noise(token, mtl=3, cs=False):\n",
    "    # ARGUMENTS: token; a spacy token\n",
    "    # optional: mtl (minimum token length); an int\n",
    "    # optional: cs (custom stop); a boolead\n",
    "    # OUTPUTS: T/F\n",
    "    #\n",
    "    is_noise = False\n",
    "    # this function performs a series of checks to see if \n",
    "    # a token is noise and then returns t/f\n",
    "    # essentially it's a giant switch\n",
    "    #\n",
    "    #\n",
    "    noisy_pos_tags = [\"PROP\", \"PRON\"]\n",
    "    #\n",
    "    # here's a regular expression for matching dates/times from a string\n",
    "    # spacy doesn't handle that task well\n",
    "    dates = re.compile('\\d{1,2}(?P<sep>[-/])\\d{1,2}(?P=sep)\\d{2,4}')\n",
    "    times = re.compile(r'\\d{1,2}(:\\d{1,2})?(am|pm)?')\n",
    "    #\n",
    "    # these are some stop words that occur pretty frequently across docs;\n",
    "    # it might make sense to expand these further\n",
    "    custom_stop_words = ['this', 'that', 'please', 'be', 'file',\n",
    "                         'copy', 'with', 'from', 'document', 'like',\n",
    "                         'have', 'other', 'thank', 'and/or', 'record']\n",
    "    #\n",
    "    #\n",
    "    # cs refers to 'custom stop', and adds the stopwords to the spacy\n",
    "    # list if true\n",
    "    if cs == True:\n",
    "        for stopword in custom_stop_words:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "    #\n",
    "    # measures length of token; default is 3\n",
    "    if len(token.text) <= mtl:\n",
    "        is_noise = True\n",
    "    # filters based on list above\n",
    "    elif token.pos_ in noisy_pos_tags:\n",
    "        is_noise = True\n",
    "    elif token.text == '-PRON-':\n",
    "        is_noise = True\n",
    "    elif bool(dates.findall(token.text)) == True:\n",
    "        is_noise = True\n",
    "    elif bool(times.findall(token.text)) == True:\n",
    "        is_noise = True\n",
    "    # filters stop words\n",
    "    elif token in STOP_WORDS:\n",
    "        is_noise = True\n",
    "    elif token.is_stop == True:\n",
    "        is_noise = True\n",
    "    # filters things that are/look like numbers\n",
    "    elif token.is_digit == True:\n",
    "        is_noise = True\n",
    "    elif token.is_currency == True:\n",
    "        is_noise = True\n",
    "    elif token.like_num == True:\n",
    "        is_noise = True\n",
    "    # filters web stuff\n",
    "    elif token.like_url == True:\n",
    "        is_noise = True\n",
    "    elif token.like_email == True:\n",
    "        is_noise = True\n",
    "    # filters punctuation\n",
    "    elif token.is_punct == True:\n",
    "        is_noise = True\n",
    "    elif token.is_left_punct == True:\n",
    "        is_noise = True\n",
    "    elif token.is_right_punct == True:\n",
    "        is_noise = True\n",
    "    elif token.is_bracket == True:\n",
    "        is_noise = True\n",
    "    elif token.is_quote == True:\n",
    "        is_noise = True\n",
    "    elif token.is_space == True:\n",
    "        is_noise = True\n",
    "    return is_noise \n",
    "\n",
    "\n",
    "def lem_stop(text, l=True):\n",
    "    # ARGUMENTS: text; a list containing lists, containing spacy tokens\n",
    "    # l; a boolean flag, True by default\n",
    "    # OUTPUTS: a list containing lists, containing lemmas of tokens\n",
    "    #\n",
    "    # empty list, later to become a list of lists\n",
    "    txt = []\n",
    "    #\n",
    "    for sentences in text:\n",
    "        if l == True:\n",
    "            # if the default isn't changed,\n",
    "            # check to see if a word is noise; if not, keep the lemma form in the list\n",
    "            sent = [token.lemma_ for token in sentences if filter_noise(token) == False]\n",
    "        else:\n",
    "            # if the defaults are different, just add the complete token\n",
    "            sent = [token for token in sentences if filter_noise(token) == False] \n",
    "        # then append the sentence to the list\n",
    "        txt.append(sent)\n",
    "    return txt\n",
    "\n",
    "def ner(text):\n",
    "    # ARGUMENTS: text; a list containing lists, containing spacy tokens\n",
    "    # OUTPUTS: a list containing lists, containing named entities\n",
    "    #\n",
    "    # empty list, later to become a list of lists\n",
    "    txt = []\n",
    "    #\n",
    "    for sentences in text:\n",
    "        sent = [entity for entity in sentences.ents] \n",
    "        # then append the sentence to the list\n",
    "        txt.append(sent)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# this line takes all the text in the request summary column\n",
    "# and extracts each entry into a list\n",
    "o = sentences(spd, 'request_summary')\n",
    "\n",
    "# extract the information, producing a list of sentences (really, a list of tokens)\n",
    "docs = nlp_proc(o)\n",
    "\n",
    "# finally, we:\n",
    "# remove stopwords\n",
    "# remove punctuation\n",
    "# lemmatize (reduce to simplest form for purposes of similarity)\n",
    "\n",
    "texts = lem_stop(docs)\n",
    "\n",
    "end = time.time()\n",
    "# this should take approximately 10min to run on the full seattle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.81355595588684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(TRN)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Mobile)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(the, City)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(the, the, City, Centre)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(DPD, Project)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(4624)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(S.)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Dates):9510, 28TH)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Ryan, C., Nute)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(AVE, NW)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(AVE, NW)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(SEATTLE)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Seattle)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(1)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(5, -, 20)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(21, -, 50)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(50)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(27TH)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(Ave, NW, Seattle, WA9507)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(Tariq, Judeh)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(the, Seattle, Fire, Department)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(Aug, 24, ,, 2016, to, Aug, 24, ,, 2017)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(George, Scarola)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(Aug, 24)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(201)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(Graham)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(6056)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(Martin, Luther, King)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(10.7.14)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(9, -, 21, -, 2017)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41145</th>\n",
       "      <td>(Ballard, Oil)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41146</th>\n",
       "      <td>(North, Seattle, Industrial, Association)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41147</th>\n",
       "      <td>(Burke, Gilman)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41148</th>\n",
       "      <td>(Mediation)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41149</th>\n",
       "      <td>(Murray)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41150</th>\n",
       "      <td>(Burke, -, Gilman, Trail)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41151</th>\n",
       "      <td>(February, 28, ,, 2017)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>(Salmon, Bay, Sand, &amp;, Gravel)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>(Sewer, &amp;, Stormwater, Video, (, CCTV)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>(Fire, Report)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>(TX)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>(Underground, Storage, Tanks)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41157</th>\n",
       "      <td>(the, City, of, Seattle, ,)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41158</th>\n",
       "      <td>(Finance, and, Administrative, Services, ,, Bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41159</th>\n",
       "      <td>(2006, to, 2016)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41160</th>\n",
       "      <td>(I., \\t, Aggregate, fileI)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41161</th>\n",
       "      <td>(2006)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41162</th>\n",
       "      <td>(2016)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41163</th>\n",
       "      <td>(4,015)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41164</th>\n",
       "      <td>(365, days)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41165</th>\n",
       "      <td>(11, years)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41166</th>\n",
       "      <td>(monthly)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41167</th>\n",
       "      <td>(2006)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41168</th>\n",
       "      <td>(2016)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41169</th>\n",
       "      <td>(XY)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41170</th>\n",
       "      <td>(XY)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41171</th>\n",
       "      <td>(Excel)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41172</th>\n",
       "      <td>(KrebsGraduate, Research, AssistantThe, Univer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41173</th>\n",
       "      <td>(98144)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41174</th>\n",
       "      <td>(4483)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41175 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   index  count\n",
       "0                                                  (TRN)      1\n",
       "1                                               (Mobile)      1\n",
       "2                                            (the, City)      1\n",
       "3                               (the, the, City, Centre)      1\n",
       "4                                         (DPD, Project)      1\n",
       "5                                                 (4624)      1\n",
       "6                                                   (S.)      1\n",
       "7                                    (Dates):9510, 28TH)      1\n",
       "8                                       (Ryan, C., Nute)      1\n",
       "9                                              (AVE, NW)      1\n",
       "10                                             (AVE, NW)      1\n",
       "11                                             (SEATTLE)      1\n",
       "12                                             (Seattle)      1\n",
       "13                                                   (1)      1\n",
       "14                                            (5, -, 20)      1\n",
       "15                                           (21, -, 50)      1\n",
       "16                                                  (50)      1\n",
       "17                                                (27TH)      1\n",
       "18                            (Ave, NW, Seattle, WA9507)      1\n",
       "19                                        (Tariq, Judeh)      1\n",
       "20                      (the, Seattle, Fire, Department)      1\n",
       "21              (Aug, 24, ,, 2016, to, Aug, 24, ,, 2017)      1\n",
       "22                                     (George, Scarola)      1\n",
       "23                                             (Aug, 24)      1\n",
       "24                                                 (201)      1\n",
       "25                                              (Graham)      1\n",
       "26                                                (6056)      1\n",
       "27                                (Martin, Luther, King)      1\n",
       "28                                             (10.7.14)      1\n",
       "29                                   (9, -, 21, -, 2017)      1\n",
       "...                                                  ...    ...\n",
       "41145                                     (Ballard, Oil)      1\n",
       "41146          (North, Seattle, Industrial, Association)      1\n",
       "41147                                    (Burke, Gilman)      1\n",
       "41148                                        (Mediation)      1\n",
       "41149                                           (Murray)      1\n",
       "41150                          (Burke, -, Gilman, Trail)      1\n",
       "41151                            (February, 28, ,, 2017)      1\n",
       "41152                     (Salmon, Bay, Sand, &, Gravel)      1\n",
       "41153             (Sewer, &, Stormwater, Video, (, CCTV)      1\n",
       "41154                                     (Fire, Report)      1\n",
       "41155                                               (TX)      1\n",
       "41156                      (Underground, Storage, Tanks)      1\n",
       "41157                        (the, City, of, Seattle, ,)      1\n",
       "41158  (Finance, and, Administrative, Services, ,, Bu...      1\n",
       "41159                                   (2006, to, 2016)      1\n",
       "41160                         (I., \\t, Aggregate, fileI)      1\n",
       "41161                                             (2006)      1\n",
       "41162                                             (2016)      1\n",
       "41163                                            (4,015)      1\n",
       "41164                                        (365, days)      1\n",
       "41165                                        (11, years)      1\n",
       "41166                                          (monthly)      1\n",
       "41167                                             (2006)      1\n",
       "41168                                             (2016)      1\n",
       "41169                                               (XY)      1\n",
       "41170                                               (XY)      1\n",
       "41171                                            (Excel)      1\n",
       "41172  (KrebsGraduate, Research, AssistantThe, Univer...      1\n",
       "41173                                            (98144)      1\n",
       "41174                                             (4483)      1\n",
       "\n",
       "[41175 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(end-start)\n",
    "countents = Counter(itertools.chain(*ents))\n",
    "edf = pd.DataFrame.from_dict(countents, orient='index', columns=['count'])\n",
    "edf = edf.sort_values(by='count', ascending=True).reset_index()\n",
    "edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>pct</th>\n",
       "      <th>prp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>record</th>\n",
       "      <td>4431</td>\n",
       "      <td>0.020289</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request</th>\n",
       "      <td>4305</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.971564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seattle</th>\n",
       "      <td>3982</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.898668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>2757</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.622207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>include</th>\n",
       "      <td>2225</td>\n",
       "      <td>0.010188</td>\n",
       "      <td>0.502144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public</th>\n",
       "      <td>2126</td>\n",
       "      <td>0.009735</td>\n",
       "      <td>0.479801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storage</th>\n",
       "      <td>2016</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>0.454976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email</th>\n",
       "      <td>1983</td>\n",
       "      <td>0.009080</td>\n",
       "      <td>0.447529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>underground</th>\n",
       "      <td>1892</td>\n",
       "      <td>0.008663</td>\n",
       "      <td>0.426992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanks</th>\n",
       "      <td>1784</td>\n",
       "      <td>0.008169</td>\n",
       "      <td>0.402618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decommission</th>\n",
       "      <td>1734</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>0.391334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>please</th>\n",
       "      <td>1732</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.390882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relate</th>\n",
       "      <td>1645</td>\n",
       "      <td>0.007532</td>\n",
       "      <td>0.371248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property</th>\n",
       "      <td>1612</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information</th>\n",
       "      <td>1594</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.359738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>1545</td>\n",
       "      <td>0.007074</td>\n",
       "      <td>0.348680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provide</th>\n",
       "      <td>1541</td>\n",
       "      <td>0.007056</td>\n",
       "      <td>0.347777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>report</th>\n",
       "      <td>1484</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>0.334913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>1462</td>\n",
       "      <td>0.006694</td>\n",
       "      <td>0.329948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regard</th>\n",
       "      <td>1422</td>\n",
       "      <td>0.006511</td>\n",
       "      <td>0.320921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>1345</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.303543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>1234</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.278492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>department</th>\n",
       "      <td>1091</td>\n",
       "      <td>0.004996</td>\n",
       "      <td>0.246220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-PRON-</th>\n",
       "      <td>1069</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>0.241255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>copy</th>\n",
       "      <td>1022</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>0.230648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>between</th>\n",
       "      <td>1018</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.229745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thank</th>\n",
       "      <td>994</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>0.224329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>permit</th>\n",
       "      <td>979</td>\n",
       "      <td>0.004483</td>\n",
       "      <td>0.220943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site</th>\n",
       "      <td>877</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.197924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assessment</th>\n",
       "      <td>855</td>\n",
       "      <td>0.003915</td>\n",
       "      <td>0.192959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>846</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.190928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>827</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>0.186640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>office</th>\n",
       "      <td>811</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.183029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environmental</th>\n",
       "      <td>803</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.181223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>street</th>\n",
       "      <td>797</td>\n",
       "      <td>0.003649</td>\n",
       "      <td>0.179869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communication</th>\n",
       "      <td>793</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.178966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avenue</th>\n",
       "      <td>762</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.171970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>follow</th>\n",
       "      <td>756</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>0.170616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violation</th>\n",
       "      <td>721</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.162717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>694</td>\n",
       "      <td>0.003178</td>\n",
       "      <td>0.156624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>these</th>\n",
       "      <td>688</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.155270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building</th>\n",
       "      <td>669</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.150982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <td>666</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.150305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complaint</th>\n",
       "      <td>644</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.145340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>send</th>\n",
       "      <td>641</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.144663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>624</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.140826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address</th>\n",
       "      <td>620</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.139923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>available</th>\n",
       "      <td>609</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.137441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>603</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>0.136087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>limit</th>\n",
       "      <td>598</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.134958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count       pct       prp\n",
       "record          4431  0.020289  1.000000\n",
       "request         4305  0.019712  0.971564\n",
       "seattle         3982  0.018233  0.898668\n",
       "city            2757  0.012624  0.622207\n",
       "include         2225  0.010188  0.502144\n",
       "public          2126  0.009735  0.479801\n",
       "storage         2016  0.009231  0.454976\n",
       "email           1983  0.009080  0.447529\n",
       "underground     1892  0.008663  0.426992\n",
       "tanks           1784  0.008169  0.402618\n",
       "decommission    1734  0.007940  0.391334\n",
       "please          1732  0.007931  0.390882\n",
       "relate          1645  0.007532  0.371248\n",
       "property        1612  0.007381  0.363800\n",
       "information     1594  0.007299  0.359738\n",
       "fire            1545  0.007074  0.348680\n",
       "provide         1541  0.007056  0.347777\n",
       "report          1484  0.006795  0.334913\n",
       "document        1462  0.006694  0.329948\n",
       "regard          1422  0.006511  0.320921\n",
       "would           1345  0.006159  0.303543\n",
       "date            1234  0.005650  0.278492\n",
       "department      1091  0.004996  0.246220\n",
       "-PRON-          1069  0.004895  0.241255\n",
       "copy            1022  0.004680  0.230648\n",
       "between         1018  0.004661  0.229745\n",
       "thank            994  0.004551  0.224329\n",
       "permit           979  0.004483  0.220943\n",
       "site             877  0.004016  0.197924\n",
       "assessment       855  0.003915  0.192959\n",
       "project          846  0.003874  0.190928\n",
       "be               827  0.003787  0.186640\n",
       "office           811  0.003714  0.183029\n",
       "environmental    803  0.003677  0.181223\n",
       "street           797  0.003649  0.179869\n",
       "communication    793  0.003631  0.178966\n",
       "avenue           762  0.003489  0.171970\n",
       "follow           756  0.003462  0.170616\n",
       "violation        721  0.003301  0.162717\n",
       "number           694  0.003178  0.156624\n",
       "these            688  0.003150  0.155270\n",
       "building         669  0.003063  0.150982\n",
       "code             666  0.003050  0.150305\n",
       "complaint        644  0.002949  0.145340\n",
       "send             641  0.002935  0.144663\n",
       "this             624  0.002857  0.140826\n",
       "address          620  0.002839  0.139923\n",
       "available        609  0.002789  0.137441\n",
       "time             603  0.002761  0.136087\n",
       "limit            598  0.002738  0.134958"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(itertools.chain(*texts))\n",
    "cdf = pd.DataFrame.from_dict(counter, orient='index', columns=['count'])\n",
    "\n",
    "# percentage term frequency compared to rest of document\n",
    "cdf['pct'] = cdf['count'] / sum(cdf['count'])\n",
    "\n",
    "# normalization to reweight the \"importance\" of a word, invariate to document size\n",
    "cdf['prp'] = cdf['count'] / cdf['count'].max()\n",
    "\n",
    "cdf = cdf.sort_values(by='count', ascending=False)\n",
    "cdf.head(50)\n",
    "#prr2vec_data = [ lem_stop(i) for i in docs ]\n",
    "# the output here is a list of all of the processed, filtered text\n",
    "#\n",
    "# this is wrapped in a list in order to help word2vec work with the word embeddings\n",
    "# rather than just the characters of the text. normally word2vec takes sentences\n",
    "# rather than just words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = Dictionary(texts)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(txt) for txt in texts]\n",
    "\n",
    "dictionary = Dictionary(texts) \n",
    "passes = 45\n",
    "rs = 7\n",
    "num_topics = 10\n",
    "\n",
    "lda_40 = ldamodel.LdaModel(corpus, \n",
    "                        num_topics=num_topics, \n",
    "                        id2word = id2word, \n",
    "                        passes=passes, \n",
    "                        random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Clustering using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Latent Dirichlet Allocation (LDA)?\n",
    "Latent dirichlet allocation (LDA) is a tool for finding implicit relationships in a large body of text. The algorithm produces topics, which essentially are groupings of words that we--statistically speaking--expect to have some degree of association (represented through co-occurrence). As such, topics are not explicitly related through semantics or knowledge content--it is through later inference that we understand the emergent higher-level categories.\n",
    "\n",
    "For example, consider a body of text that contains keywords like 'China', 'black', 'white', 'spotted', 'Croatia', 'cute', and 'bamboo'. It could be the case that a subset of these words are explicitly (and exclusively) related to a particular category, while others occur within each category with about the same frequency distribution.\n",
    "\n",
    "Using LDA, we are able to categorize all of these words into groups that make sense. The computer groups 'China', 'bamboo', 'black', 'white', and 'cute' together, and our human inference suggests *'panda'*. In contrast, if we see 'Croatia', 'spotted', 'black', 'white', and 'cute', in the computer output, we think *'dalmatian'*.\n",
    "\n",
    "More complicated algorithms can be used to assign actual labels to topic categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.031*\"violation\" + 0.028*\"avenue\" + 0.028*\"code\" + 0.028*\"complaint\" + '\n",
      "  '0.025*\"please\" + 0.023*\"property\" + 0.022*\"seattle\" + 0.021*\"fire\" + '\n",
      "  '0.020*\"provide\" + 0.019*\"copy\"'),\n",
      " (1,\n",
      "  '0.059*\"record\" + 0.038*\"public\" + 0.027*\"request\" + 0.019*\"development\" + '\n",
      "  '0.018*\"produce\" + 0.018*\"project\" + 0.018*\"shoreline\" + 0.015*\"sdot\" + '\n",
      "  '0.015*\"permit\" + 0.015*\"exemption\"'),\n",
      " (2,\n",
      "  '0.044*\"report\" + 0.035*\"relate\" + 0.029*\"include\" + 0.029*\"fire\" + '\n",
      "  '0.026*\"record\" + 0.019*\"document\" + 0.018*\"incident\" + 0.017*\"seattle\" + '\n",
      "  '0.012*\"limit\" + 0.011*\"case\"'),\n",
      " (3,\n",
      "  '0.053*\"property\" + 0.049*\"site\" + 0.046*\"assessment\" + '\n",
      "  '0.044*\"environmental\" + 0.024*\"record\" + 0.022*\"seattle\" + '\n",
      "  '0.021*\"information\" + 0.020*\"permit\" + 0.018*\"request\" + 0.017*\"building\"'),\n",
      " (4,\n",
      "  '0.020*\"would\" + 0.015*\"request\" + 0.013*\"be\" + 0.012*\"information\" + '\n",
      "  '0.011*\"seattle\" + 0.011*\"thank\" + 0.010*\"look\" + 0.009*\"number\" + '\n",
      "  '0.009*\"street\" + 0.009*\"-PRON-\"'),\n",
      " (5,\n",
      "  '0.043*\"city\" + 0.041*\"seattle\" + 0.020*\"include\" + 0.014*\"relate\" + '\n",
      "  '0.014*\"department\" + 0.013*\"record\" + 0.013*\"between\" + 0.011*\"document\" + '\n",
      "  '0.010*\"request\" + 0.009*\"provide\"'),\n",
      " (6,\n",
      "  '0.056*\"request\" + 0.025*\"record\" + 0.023*\"public\" + 0.018*\"please\" + '\n",
      "  '0.015*\"information\" + 0.013*\"copy\" + 0.012*\"-PRON-\" + 0.011*\"send\" + '\n",
      "  '0.010*\"provide\" + 0.010*\"available\"'),\n",
      " (7,\n",
      "  '0.075*\"email\" + 0.033*\"unrecognized\" + 0.017*\"provide\" + 0.014*\"calendar\" + '\n",
      "  '0.013*\"meeting\" + 0.011*\"city\" + 0.008*\"proposal\" + 0.007*\"limitation\" + '\n",
      "  '0.007*\"extent\" + 0.007*\"include\"'),\n",
      " (8,\n",
      "  '0.174*\"underground\" + 0.173*\"storage\" + 0.162*\"tanks\" + '\n",
      "  '0.157*\"decommission\" + 0.031*\"trail\" + 0.030*\"burke\" + 0.030*\"missing\" + '\n",
      "  '0.030*\"gilman\" + 0.007*\"forth\" + 0.007*\"table\"'),\n",
      " (9,\n",
      "  '0.026*\"record\" + 0.019*\"fire\" + 0.014*\"current\" + 0.014*\"these\" + '\n",
      "  '0.013*\"regard\" + 0.013*\"office\" + 0.012*\"property\" + 0.012*\"request\" + '\n",
      "  '0.011*\"date\" + 0.011*\"international\"')]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(lda_40.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Cosine Similarity Using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Word2Vec?\n",
    "\n",
    "Word2vec is an algorithmic system used to produce word embeddings, which may need some explanation or unpacking. Think back to the LDA section of this document. The computer produced a clustering of words, and we used our associative human creativity to establish patterns and relationships between them. What if it were possible to determine the similarity or semantic relationship between words programmatically?\n",
    "\n",
    "Word2vec achieves this by taking a large body of text and representing it as a vector space. Each word contained within that vector space is encoded as a vector, comprised of a 1 where the word is and 0's everywhere else). There is a hidden filter layer which compresses the size of this vector while minimizing information loss, as smaller vectors are less computationally complex to compare. Finally, the word vectors are all positioned in the vector space relative to each other, with more similar words clustered together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite abstract, so let's try out an example. Assume that we have the sentences \"Bananas and apples are delicious,\" and \"Durian and jackfruit are unpleasant.\" We can represent each of these as a list of words:\n",
    "\n",
    "`document1 = ['Bananas', 'and', 'apples', 'are', 'delicious', '.']`\n",
    "\n",
    "`document2 = ['Durian', 'and', 'jackfruit', 'are', 'unpleasant', '.']`\n",
    "\n",
    "And then as a vector space, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bananas       [1, 0]\n",
       "durian        [0, 1]\n",
       "and           [1, 1]\n",
       "apples        [1, 0]\n",
       "jackfruit     [0, 1]\n",
       "are           [1, 1]\n",
       "delicious     [1, 0]\n",
       "unpleasant    [0, 1]\n",
       ".             [1, 1]\n",
       "dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'bananas': [1, 0], 'durian': [0, 1], 'and': [1, 1],\n",
    "     'apples': [1, 0], 'jackfruit': [0, 1], 'are': [1, 1],\n",
    "    'delicious': [1, 0], 'unpleasant': [0, 1], '.': [1, 1]}\n",
    "\n",
    "s  = pd.Series(d,index=d.keys())\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have produced an excellent vector space here, we can make the vector space more sparse and easier to work with by dropping items that do not have semantic relevance. That includes words like 'and' and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bananas       [1, 0]\n",
       "durian        [0, 1]\n",
       "apples        [1, 0]\n",
       "jackfruit     [0, 1]\n",
       "delicious     [1, 0]\n",
       "unpleasant    [0, 1]\n",
       "dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = {'bananas': [1, 0], 'durian': [0, 1],\n",
    "     'apples': [1, 0], 'jackfruit': [0, 1],\n",
    "    'delicious': [1, 0], 'unpleasant': [0, 1]}\n",
    "\n",
    "t  = pd.Series(e,index=e.keys())\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is our vector *space*. A *word vector* is the representation of the word with regard to the entire vecor space. The following word vectors are represented like this, relative to their presence within the vector space, and the documents in which they appear:\n",
    "\n",
    "`bananas = [1, 0, 0, 0, 0, 0]`\n",
    "\n",
    "`durian = [0, 1, 0, 0, 0, 0]`\n",
    "\n",
    "`delicious = [0, 0, 0, 0, 1, 0]`\n",
    "\n",
    "`unpleasant = [0, 0, 0, 0, 0, 1]`\n",
    "\n",
    "This is a simplistic picture of how a word vector operates. There is little insight that we can derive from this, other than comparing direct equivalence of vectors. But things become interesting when there is a significantly large corpus of documents, with different uses and contexts for words.\n",
    "\n",
    "Word2vec takes word vectors for every word that appears in a corpus (as above) and represents their contexts as a series of weights. Think of it like creating a dictionary, where each definition is composed of a little piece of every other definition, but to varying degrees. Because each definition of a word is created relationally, it is possible to capture conceptual or syntactic meaning an a really robust, fascinating (almost surprising) way.\n",
    "\n",
    "Supposing we had more data (a huge set of other documents), the previous word vectors might be transformed to contain all of the \"definitions\" of the other words within the dataset:\n",
    "\n",
    "`bananas = [0.89123, 0.66545, 0.19842, 0.11901, 0.09113, 0.07221]`\n",
    "\n",
    "`durian = [0.12311, 0.71834, 0.22142, 0.13452, 0.08721, 0.067881]`\n",
    "\n",
    "`delicious = [0.13317, 0.17004, 0.21891, 0.66311, 0.88313, 0.70019]`\n",
    "\n",
    "`unpleasant = [0.14141, 0.16167, 0.22212, 0.57719, 0.77311, 0.87123]`\n",
    "\n",
    "Other algorithms can be used to reduce the dimensionality of the vector space, such that each of these vectors can be plotted in 2D space. For a naive explanation of how this works, check out the bolded components of each of these vectors:\n",
    "\n",
    "`bananas = [`**0.89123`, `0.66545**`, 0.19842, 0.11901, 0.09113, 0.07221]`\n",
    "\n",
    "`durian = [`**0.71311`, `0.86834**`, 0.22142, 0.13452, 0.08721, 0.067881]`\n",
    "\n",
    "`delicious = [0.13317, 0.17004, 0.21891, 0.38311, `**0.88313`, `0.70019**`]`\n",
    "\n",
    "`unpleasant = [0.14141, 0.16167, 0.22212, 0.57719, `**0.77311`, `0.87123**`]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "#\n",
    "# error messages of note, in case further problems arise:\n",
    "# https://stackoverflow.com/questions/33989826/python-gensim-runtimeerror-you-must-first-build-vocabulary-before-training-th/33991111\n",
    "# \n",
    "\n",
    "# the number of dimensions of generated vectors. this is a good number to\n",
    "# play around with. some people suggest square-root length of vocabulary\n",
    "# conceptually this might map onto principle components, or number of topics\n",
    "size = 50\n",
    "\n",
    "# terms that occur less than min_count number \n",
    "# of times are ignored in calculations\n",
    "# may want to change this depending on reimplementation of\n",
    "# lem_stop function above\n",
    "min_count = 1\n",
    "\n",
    "# terms that occur within this window of text are associated with it\n",
    "# during the training of the model. if the corpus of text contains large\n",
    "# sentences then it may be a good idea to change this to something larger.\n",
    "# the documentation suggests 10 as an upper bound and 4-7 as a good range.\n",
    "window = 4\n",
    "\n",
    "# skip-gram technique: boolean that determines skipgram vs continuous bag of words\n",
    "# model. the default is 1, skipgram\n",
    "sg = 1\n",
    "\n",
    "prr2vec = Word2Vec(\n",
    "    [prr2vec_data],\n",
    "    sg=sg,\n",
    "    size=size,\n",
    "    min_count=min_count,\n",
    "    window=window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sincerely', 0.9992898106575012),\n",
       " ('act', 0.9992607235908508),\n",
       " ('category', 0.9992291331291199),\n",
       " ('which', 0.9991952180862427),\n",
       " ('each', 0.9991693496704102),\n",
       " (\"'s\", 0.9991583228111267),\n",
       " ('khandelwal@kingcounty', 0.9991531372070312),\n",
       " ('boise', 0.9991462230682373),\n",
       " ('be', 0.9991364479064941),\n",
       " ('in', 0.9991249442100525)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prr2vec.wv['zone']\n",
    "#prr2vec.wv.most_similar('state')\n",
    "#prr2vec.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/spacy/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TSNE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-0d4094271860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtsne_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprr2vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-6af7c76e2364>\u001b[0m in \u001b[0;36mtsne_plot\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtsne_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TSNE' is not defined"
     ]
    }
   ],
   "source": [
    "tsne_plot(prr2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ONCE we have vectors\n",
    "#step 3 - build model\n",
    "#3 main tasks that vectors help with\n",
    "#DISTANCE, SIMILARITY, RANKING\n",
    "\n",
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions, more computationally expensive to train\n",
    "#but also more accurate\n",
    "#more dimensions = more generalized\n",
    "num_features = 300\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "prr2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=2,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot sort vocabulary after model weights already initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-9c81d02e7eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprr2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/spacy/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         report_values = self.vocabulary.prepare_vocab(\n\u001b[1;32m    483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             trim_rule=trim_rule, **kwargs)\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/spacy/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mprepare_vocab\u001b[0;34m(self, hs, negative, wv, update, keep_raw_vocab, trim_rule, min_count, sample, dry_run)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_vocab\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0;31m# add info about each word's Huffman encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/spacy/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36msort_vocab\u001b[0;34m(self, wv)\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;34m\"\"\"Sort the vocabulary so the most frequent words have the lowest indexes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot sort vocabulary after model weights already initialized.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m         \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot sort vocabulary after model weights already initialized."
     ]
    }
   ],
   "source": [
    "prr2vec.build_vocab(docs.text)\n",
    "\n",
    "token_count = sum([len(doc) for doc in docs.text])\n",
    "print(\"The corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must specify an explict epochs count. The usual value is epochs=model.epochs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-58043653cd44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprr2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprr2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprr2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/spacy/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/spacy/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/spacy/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/spacy/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m             )\n\u001b[1;32m    618\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must specify an explict epochs count. The usual value is epochs=model.epochs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         logger.info(\n\u001b[1;32m    621\u001b[0m             \u001b[0;34m\"training model with %i workers on %i vocabulary and %i features, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You must specify an explict epochs count. The usual value is epochs=model.epochs."
     ]
    }
   ],
   "source": [
    "prr2vec.train(docs.text, total_examples=prr2vec.corpus_count, epochs=prr2vec.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['requesting',\n",
       " 'copies',\n",
       " 'of',\n",
       " 'hsd',\n",
       " \"'s\",\n",
       " 'service',\n",
       " 'agreements',\n",
       " 'with',\n",
       " 'compass',\n",
       " 'housing',\n",
       " 'alliance',\n",
       " 'and',\n",
       " 'solid',\n",
       " 'ground',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'correspondence',\n",
       " 'emails',\n",
       " 'and',\n",
       " 'other',\n",
       " 'documents',\n",
       " 'that',\n",
       " 'reference',\n",
       " 'beatrice',\n",
       " 'holbert',\n",
       " 'carolyn',\n",
       " 'kinniebrow',\n",
       " 'or',\n",
       " 'carolyn',\n",
       " 'bilal',\n",
       " 'reports',\n",
       " 'regarding',\n",
       " 'financial',\n",
       " 'reviews',\n",
       " 'of',\n",
       " 'share',\n",
       " 'in',\n",
       " '2009',\n",
       " '2011',\n",
       " 'and',\n",
       " '2013from',\n",
       " 'september',\n",
       " '2010',\n",
       " 'all',\n",
       " 'email',\n",
       " 'correspondence',\n",
       " 'from',\n",
       " 'anyone',\n",
       " 'at',\n",
       " 'hsd',\n",
       " 'that',\n",
       " 'include',\n",
       " 'any',\n",
       " 'of',\n",
       " 'the',\n",
       " 'terms',\n",
       " 'below',\n",
       " 'peggy',\n",
       " 'hotes”“scott',\n",
       " 'morrow',\n",
       " 'nate',\n",
       " 'martin',\n",
       " 'share',\n",
       " 'wheel',\n",
       " 'seattle',\n",
       " 'housing',\n",
       " 'and',\n",
       " 'resource',\n",
       " 'effort\"“marvin',\n",
       " 'futrell”\"low',\n",
       " 'income',\n",
       " 'housing',\n",
       " 'institute\"\"michelle',\n",
       " 'marchand\"\"steven',\n",
       " 'isaacson\"\"lantz',\n",
       " 'rowland',\n",
       " 'jarvis',\n",
       " 'capucion',\n",
       " 'nickelsville\"\"sharon',\n",
       " 'lee\"\"tent',\n",
       " 'city\"(1',\n",
       " 'documents',\n",
       " 'and',\n",
       " 'communications',\n",
       " 'that',\n",
       " 'will',\n",
       " 'help',\n",
       " 'me',\n",
       " 'to',\n",
       " 'ascertain',\n",
       " 'how',\n",
       " 'many',\n",
       " 'other',\n",
       " 'individuals',\n",
       " 'may',\n",
       " 'be',\n",
       " 'experiencing',\n",
       " 'problems',\n",
       " 'enrolling',\n",
       " 'in',\n",
       " 'the',\n",
       " 'the',\n",
       " 'udp',\n",
       " 'program(2',\n",
       " 'relevant',\n",
       " 'copies',\n",
       " 'of',\n",
       " 'any',\n",
       " 'documents',\n",
       " 'communications',\n",
       " 'and',\n",
       " 'procedure',\n",
       " 'manuals',\n",
       " 'that',\n",
       " 'set',\n",
       " 'forth',\n",
       " 'the',\n",
       " 'procedure',\n",
       " 'for',\n",
       " 'enrolling',\n",
       " 'applicants',\n",
       " 'in',\n",
       " 'the',\n",
       " 'utility',\n",
       " 'discount',\n",
       " 'program',\n",
       " 'including',\n",
       " 'the',\n",
       " 'application',\n",
       " 'process',\n",
       " 'itself(3',\n",
       " 'all',\n",
       " 'documents',\n",
       " 'and',\n",
       " 'communications',\n",
       " 'that',\n",
       " 'relate',\n",
       " 'to',\n",
       " 'the',\n",
       " 'erroneous',\n",
       " 'statement',\n",
       " 'of',\n",
       " 'my',\n",
       " 'current',\n",
       " 'rent',\n",
       " 'in',\n",
       " 'my',\n",
       " 'application',\n",
       " 'the',\n",
       " 'application',\n",
       " 'states',\n",
       " 'it',\n",
       " 'is',\n",
       " '295.00',\n",
       " 'month',\n",
       " 'when',\n",
       " 'clearly',\n",
       " 'told',\n",
       " 'seattle',\n",
       " 'city',\n",
       " 'light',\n",
       " 'that',\n",
       " 'pay',\n",
       " '395.00',\n",
       " 'in',\n",
       " 'monthly',\n",
       " 'rent(4',\n",
       " 'all',\n",
       " 'documents',\n",
       " 'and',\n",
       " 'communications',\n",
       " 'to',\n",
       " 'and',\n",
       " 'from',\n",
       " 'sha',\n",
       " 're',\n",
       " 'eligibility',\n",
       " 'of',\n",
       " 'sha',\n",
       " 'tenants',\n",
       " 'or',\n",
       " 'housing',\n",
       " 'choice',\n",
       " 'voucher',\n",
       " 'holders',\n",
       " 'for',\n",
       " 'the',\n",
       " 'udpre',\n",
       " 'public',\n",
       " 'records',\n",
       " 'request',\n",
       " 'magnolia',\n",
       " 'bridge',\n",
       " 'holland',\n",
       " 'nathan',\n",
       " 'christopher',\n",
       " 'sex',\n",
       " 'offender',\n",
       " 'registration',\n",
       " '8017dear',\n",
       " 'wa',\n",
       " 'dept',\n",
       " 'of',\n",
       " 'ecology',\n",
       " 'king',\n",
       " 'county',\n",
       " 'sex',\n",
       " 'offender',\n",
       " 'unit',\n",
       " 'seattle',\n",
       " 'police',\n",
       " 'dept',\n",
       " 'sex',\n",
       " 'offender',\n",
       " 'detail',\n",
       " 'mayor',\n",
       " 'murray',\n",
       " 'council',\n",
       " 'seattle',\n",
       " 'human',\n",
       " 'services',\n",
       " 'dept',\n",
       " 'and',\n",
       " 'seattle',\n",
       " 'transportation',\n",
       " 'dept.:nathan',\n",
       " 'christopher',\n",
       " 'holland',\n",
       " 'nate',\n",
       " 'holland',\n",
       " 'from',\n",
       " 'california',\n",
       " 'who',\n",
       " 'is',\n",
       " 'convicted',\n",
       " 'child',\n",
       " 'molester',\n",
       " 'felon',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'whole',\n",
       " 'other',\n",
       " 'litany',\n",
       " 'of',\n",
       " 'crimes',\n",
       " 'that',\n",
       " 'spd',\n",
       " 'forgot',\n",
       " 'to',\n",
       " 'mention',\n",
       " 'was',\n",
       " 'on',\n",
       " 'washington',\n",
       " '’s',\n",
       " 'most',\n",
       " 'wanted',\n",
       " 'sex',\n",
       " 'offender',\n",
       " 'hiding',\n",
       " 'out',\n",
       " 'in',\n",
       " 'the',\n",
       " 'jungle',\n",
       " 'and',\n",
       " 'now',\n",
       " 'as',\n",
       " 'of',\n",
       " 'today',\n",
       " 'friday',\n",
       " 'october',\n",
       " '2016',\n",
       " 'is',\n",
       " 'registered',\n",
       " 'as',\n",
       " 'transient',\n",
       " 'with',\n",
       " 'the',\n",
       " 'king',\n",
       " 'county',\n",
       " 'sheriff',\n",
       " '’s',\n",
       " 'office',\n",
       " 'as',\n",
       " 'residing',\n",
       " 'at',\n",
       " '1600',\n",
       " 'west',\n",
       " 'garfield',\n",
       " 'street',\n",
       " 'he',\n",
       " 'is',\n",
       " 'living',\n",
       " 'under',\n",
       " 'on',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'seattle',\n",
       " 'department',\n",
       " 'of',\n",
       " 'transportation',\n",
       " '’s',\n",
       " 'magnolia',\n",
       " 'bridge',\n",
       " 'pier',\n",
       " 'ramp',\n",
       " 'and',\n",
       " 'dumping',\n",
       " 'his',\n",
       " 'personal',\n",
       " 'human',\n",
       " 'waste',\n",
       " 'into',\n",
       " 'the',\n",
       " 'adjacent',\n",
       " 'waters',\n",
       " 'of',\n",
       " 'puget',\n",
       " 'sound',\n",
       " 'and',\n",
       " 'nearby',\n",
       " 'the',\n",
       " 'elliott',\n",
       " 'bay',\n",
       " 'bike',\n",
       " 'trail',\n",
       " 'seriously',\n",
       " 'how',\n",
       " 'can',\n",
       " 'anyone',\n",
       " 'be',\n",
       " 'allowed',\n",
       " 'to',\n",
       " 'live',\n",
       " 'under',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'seattle',\n",
       " '’s',\n",
       " 'structurally',\n",
       " 'deficient',\n",
       " 'magnolia',\n",
       " 'bridge',\n",
       " 'that',\n",
       " 'is',\n",
       " 'crumbling',\n",
       " 'deteriorated',\n",
       " 'and',\n",
       " 'has',\n",
       " 'loose',\n",
       " 'chunks',\n",
       " 'of',\n",
       " 'concrete',\n",
       " 'falling',\n",
       " 'below',\n",
       " 'that',\n",
       " 'could',\n",
       " 'kill',\n",
       " 'this',\n",
       " 'person',\n",
       " 'http://www.kiro7.com/news/bridge/246321877',\n",
       " 'specifically',\n",
       " 'who',\n",
       " 'from',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'seattle',\n",
       " 'by',\n",
       " 'name',\n",
       " 'title',\n",
       " 'department',\n",
       " 'and',\n",
       " 'by',\n",
       " 'written',\n",
       " 'authorization',\n",
       " 'consented',\n",
       " 'to',\n",
       " 'this',\n",
       " 'person',\n",
       " 'being',\n",
       " 'able',\n",
       " 'to',\n",
       " 'reside',\n",
       " 'and/or',\n",
       " 'register',\n",
       " 'at',\n",
       " '1600',\n",
       " 'west',\n",
       " 'garfield',\n",
       " 'street',\n",
       " 'and',\n",
       " 'more',\n",
       " 'specifically',\n",
       " 'is',\n",
       " 'allowing',\n",
       " 'this',\n",
       " 'person',\n",
       " 'to',\n",
       " 'dump',\n",
       " 'his',\n",
       " 'human',\n",
       " 'waste',\n",
       " 'into',\n",
       " 'the',\n",
       " 'waters',\n",
       " 'of',\n",
       " 'puget',\n",
       " 'sound',\n",
       " '',\n",
       " 'per',\n",
       " 'the',\n",
       " 'washington',\n",
       " 'public',\n",
       " 'records',\n",
       " 'request',\n",
       " 'act',\n",
       " 'please',\n",
       " 'provide',\n",
       " 'me',\n",
       " 'with',\n",
       " 'copies',\n",
       " 'by',\n",
       " 'email',\n",
       " 'of',\n",
       " 'any',\n",
       " 'and',\n",
       " 'all',\n",
       " 'correspondence',\n",
       " 'policy',\n",
       " 'permit',\n",
       " 'document',\n",
       " 'authorization',\n",
       " 'consent',\n",
       " 'land',\n",
       " 'use',\n",
       " 'permit',\n",
       " 'sepa',\n",
       " 'checklist',\n",
       " 'sepa',\n",
       " 'determination',\n",
       " 'npdes',\n",
       " 'permit',\n",
       " 'sewage',\n",
       " 'spill',\n",
       " 'dumping',\n",
       " 'reports',\n",
       " 'and',\n",
       " 'any',\n",
       " 'other',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'hereto',\n",
       " 'for',\n",
       " 'this',\n",
       " 'person',\n",
       " 'and',\n",
       " 'this',\n",
       " 'location',\n",
       " 'unsuitable',\n",
       " 'unsafe',\n",
       " 'unacceptable',\n",
       " 'but',\n",
       " 'hey',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'there',\n",
       " 'could',\n",
       " 'possibly',\n",
       " 'be',\n",
       " 'cash',\n",
       " 'reward',\n",
       " 'out',\n",
       " 'there',\n",
       " 'on',\n",
       " 'this',\n",
       " 'person',\n",
       " 'as',\n",
       " 'well',\n",
       " 'maybe',\n",
       " 'the',\n",
       " 'city',\n",
       " 'can',\n",
       " 'claim',\n",
       " 'it',\n",
       " 'to',\n",
       " 'apply',\n",
       " 'towards',\n",
       " 'the',\n",
       " 'hazmat',\n",
       " 'cleanup',\n",
       " 'costs',\n",
       " 'sincerely',\n",
       " 'concerned',\n",
       " 'jane',\n",
       " 'doegrubstello@mac.com-a',\n",
       " 'copy',\n",
       " 'of',\n",
       " 'the',\n",
       " 'expenses',\n",
       " 'for',\n",
       " 'the',\n",
       " 'pronto',\n",
       " 'bike',\n",
       " 'share',\n",
       " 'program',\n",
       " 'for',\n",
       " '2016',\n",
       " 'along',\n",
       " 'with',\n",
       " 'the',\n",
       " 'income',\n",
       " 'statement',\n",
       " 'for',\n",
       " 'the',\n",
       " 'same',\n",
       " 'period',\n",
       " '2016',\n",
       " 'through',\n",
       " '2016',\n",
       " '-a',\n",
       " 'copy',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'expenses',\n",
       " 'related',\n",
       " 'to',\n",
       " 'the',\n",
       " '50',\n",
       " 'million',\n",
       " 'dollars',\n",
       " 'budgeted',\n",
       " 'for',\n",
       " 'the',\n",
       " 'homeless',\n",
       " 'programs',\n",
       " 'from',\n",
       " '20the',\n",
       " 'most',\n",
       " 'recent',\n",
       " 'job',\n",
       " 'description',\n",
       " 'for',\n",
       " 'efren',\n",
       " 'agmata',\n",
       " 'most',\n",
       " 'recent',\n",
       " 'job',\n",
       " 'descriptions',\n",
       " 'for',\n",
       " 'two',\n",
       " 'levels',\n",
       " 'of',\n",
       " 'supervisors',\n",
       " 'above',\n",
       " 'mr.',\n",
       " 'agmata',\n",
       " 'along',\n",
       " 'with',\n",
       " 'name',\n",
       " 'and',\n",
       " 'contact',\n",
       " 'info',\n",
       " 'for',\n",
       " 'each',\n",
       " 'catherine',\n",
       " 'lester',\n",
       " '’s',\n",
       " 'calendar',\n",
       " 'for',\n",
       " 'june',\n",
       " '2016',\n",
       " 'through',\n",
       " 'december',\n",
       " 'of',\n",
       " '2016i',\n",
       " 'would',\n",
       " 'like',\n",
       " 'all',\n",
       " 'the',\n",
       " 'dates',\n",
       " 'of',\n",
       " 'site',\n",
       " 'visits',\n",
       " 'conducted',\n",
       " 'by',\n",
       " 'the',\n",
       " 'city',\n",
       " 'to',\n",
       " 'nickelsville',\n",
       " 'for',\n",
       " '2014',\n",
       " 'and',\n",
       " 'who',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'site',\n",
       " 'for',\n",
       " 'each',\n",
       " 'date',\n",
       " 'please',\n",
       " 'address',\n",
       " 'each',\n",
       " 'item',\n",
       " 'as',\n",
       " 'separate',\n",
       " 'request',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'receive',\n",
       " 'things',\n",
       " 'as',\n",
       " 'they',\n",
       " 'are',\n",
       " 'discovered',\n",
       " 'by',\n",
       " 'the',\n",
       " 'person',\n",
       " 'in',\n",
       " 'charge',\n",
       " 'of',\n",
       " 'finding',\n",
       " 'these',\n",
       " 'records',\n",
       " 'the',\n",
       " '2016',\n",
       " 'city',\n",
       " 'contract',\n",
       " 'with',\n",
       " 'share',\n",
       " 'states',\n",
       " 'that',\n",
       " 'if',\n",
       " 'reports',\n",
       " 'are',\n",
       " 'not',\n",
       " 'received',\n",
       " 'in',\n",
       " 'timely',\n",
       " 'manner',\n",
       " 'or',\n",
       " 'not',\n",
       " 'completed',\n",
       " 'invoices',\n",
       " 'will',\n",
       " 'be',\n",
       " 'held',\n",
       " 'for',\n",
       " 'payment',\n",
       " '....',\n",
       " 'does',\n",
       " 'share',\n",
       " 'have',\n",
       " 'any',\n",
       " 'invoices',\n",
       " 'being',\n",
       " 'held',\n",
       " 'currently',\n",
       " 'if',\n",
       " 'how',\n",
       " 'many',\n",
       " 'what',\n",
       " 'for',\n",
       " 'for',\n",
       " 'each',\n",
       " 'and',\n",
       " 'for',\n",
       " 'what',\n",
       " 'monetary',\n",
       " 'amount',\n",
       " 'for',\n",
       " 'each)?2',\n",
       " 'the',\n",
       " 'contract',\n",
       " 'also',\n",
       " 'states',\n",
       " 'that',\n",
       " 'the',\n",
       " 'agency',\n",
       " 'shall',\n",
       " 'maintain',\n",
       " 'client',\n",
       " 'grievance',\n",
       " 'procedures',\n",
       " '...',\n",
       " 'documentation',\n",
       " 'of',\n",
       " 'all',\n",
       " 'grievances',\n",
       " 'filed',\n",
       " 'against',\n",
       " 'the',\n",
       " 'program',\n",
       " '....',\n",
       " '\"i',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'receive',\n",
       " 'any',\n",
       " 'and',\n",
       " 'all',\n",
       " 'of',\n",
       " 'these',\n",
       " 'grievances',\n",
       " 'filed',\n",
       " 'against',\n",
       " 'share',\n",
       " 'from',\n",
       " '2013',\n",
       " 'to',\n",
       " 'present',\n",
       " 'is',\n",
       " 'mary',\n",
       " 'flowers',\n",
       " 'the',\n",
       " 'sole',\n",
       " 'overseer',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reporting',\n",
       " 'documents',\n",
       " 'share',\n",
       " 'is',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'supply?requesting',\n",
       " 'all',\n",
       " 'reports',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'the',\n",
       " 'financial',\n",
       " 'reviews',\n",
       " 'of',\n",
       " 'the',\n",
       " 'agency',\n",
       " 'known',\n",
       " 'as',\n",
       " 'share',\n",
       " 'for',\n",
       " '2009',\n",
       " '2011',\n",
       " 'and',\n",
       " '2013please',\n",
       " 'see',\n",
       " 'attached',\n",
       " 'please',\n",
       " 'see',\n",
       " 'attached',\n",
       " 'please',\n",
       " 'see',\n",
       " 'attached',\n",
       " 'records',\n",
       " 'requested',\n",
       " 'are',\n",
       " 'email',\n",
       " 'sent',\n",
       " 'to',\n",
       " 'or',\n",
       " 'from',\n",
       " 'catherine',\n",
       " 'lester',\n",
       " 'terry',\n",
       " 'mclellan',\n",
       " 'gloria',\n",
       " 'hatcher',\n",
       " 'mays',\n",
       " 'and/or',\n",
       " 'tiffany',\n",
       " 'washington',\n",
       " 'that',\n",
       " 'contain',\n",
       " 'specific',\n",
       " 'verbiage:1',\n",
       " 'yell',\n",
       " 'yells',\n",
       " 'yelled',\n",
       " 'yelling2',\n",
       " 'scream',\n",
       " 'screams',\n",
       " 'screamed',\n",
       " 'screaming3',\n",
       " 'conflict4',\n",
       " 'fight',\n",
       " 'fought',\n",
       " 'complain',\n",
       " 'complained',\n",
       " 'complaining6',\n",
       " 'argue',\n",
       " 'argues',\n",
       " 'argued',\n",
       " 'arguing',\n",
       " 'intimidate',\n",
       " 'intimidated',\n",
       " 'intimidating8.)\"workplace',\n",
       " 'expectations\"9',\n",
       " 'open',\n",
       " 'communication\"10.)\"performing',\n",
       " 'all',\n",
       " 'duties',\n",
       " 'productively\"11',\n",
       " 'probationary',\n",
       " 'discharge',\n",
       " 'discrimination',\n",
       " 'discriminates',\n",
       " 'discriminated',\n",
       " 'discriminating',\n",
       " 'hi',\n",
       " \"-i'm\",\n",
       " 'looking',\n",
       " 'for',\n",
       " 'monthly',\n",
       " 'report',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'generated',\n",
       " 'by',\n",
       " 'hsd',\n",
       " 'on',\n",
       " 'the',\n",
       " 'homeless',\n",
       " 'state',\n",
       " 'of',\n",
       " 'emergency',\n",
       " 'are',\n",
       " 'you',\n",
       " 'able',\n",
       " 'to',\n",
       " 'forward',\n",
       " 'me',\n",
       " 'each',\n",
       " 'copy',\n",
       " 'of',\n",
       " 'this',\n",
       " 'report',\n",
       " 'going',\n",
       " 'back',\n",
       " '12',\n",
       " 'months',\n",
       " 'if',\n",
       " 'collecting',\n",
       " 'all',\n",
       " 'the',\n",
       " 'copies',\n",
       " 'would',\n",
       " 'cause',\n",
       " 'delay',\n",
       " 'would',\n",
       " 'appreciate',\n",
       " 'receiving',\n",
       " 'the',\n",
       " 'most',\n",
       " 'recent',\n",
       " 'report',\n",
       " 'first',\n",
       " 'thank',\n",
       " 'you!any',\n",
       " 'and',\n",
       " 'all',\n",
       " 'monthly',\n",
       " 'quarterly',\n",
       " 'and',\n",
       " 'annual',\n",
       " 'director',\n",
       " \"'s\",\n",
       " 'reports',\n",
       " 'issued',\n",
       " 'by',\n",
       " 'the',\n",
       " 'director',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'services',\n",
       " 'department',\n",
       " 'to',\n",
       " 'the',\n",
       " 'mayor',\n",
       " 'or',\n",
       " 'any',\n",
       " 'city',\n",
       " 'council',\n",
       " 'member',\n",
       " 'from',\n",
       " 'january',\n",
       " '2016',\n",
       " 'to',\n",
       " 'january',\n",
       " '2017.1',\n",
       " 'complete',\n",
       " 'copy',\n",
       " 'of',\n",
       " 'my',\n",
       " 'personnel',\n",
       " 'file',\n",
       " 'and',\n",
       " 'any',\n",
       " 'supervisor',\n",
       " 'files',\n",
       " 'for',\n",
       " 'me2',\n",
       " 'copies',\n",
       " 'of',\n",
       " 'any',\n",
       " 'and',\n",
       " 'all',\n",
       " 'email',\n",
       " 'or',\n",
       " 'written',\n",
       " 'exchanges',\n",
       " 'between',\n",
       " 'my',\n",
       " 'supervisor',\n",
       " 'management',\n",
       " 'hr',\n",
       " 'and',\n",
       " 'employees',\n",
       " 'utilized',\n",
       " 'discussing',\n",
       " 'my',\n",
       " 'behavior',\n",
       " 'performance',\n",
       " 'and',\n",
       " 'possible',\n",
       " 'discipline3',\n",
       " 'copy',\n",
       " 'of',\n",
       " 'any',\n",
       " 'records',\n",
       " 'documentation',\n",
       " 'or',\n",
       " 'notes',\n",
       " 'for',\n",
       " 'any',\n",
       " 'investigation',\n",
       " 'related',\n",
       " 'to',\n",
       " 'my',\n",
       " 'performance',\n",
       " 'or',\n",
       " 'behavior',\n",
       " 'in',\n",
       " 'the',\n",
       " 'work',\n",
       " 'placeany',\n",
       " 'communications',\n",
       " 'correspondence',\n",
       " 'statements',\n",
       " 'opinion',\n",
       " 'letters',\n",
       " 'analyses',\n",
       " 'planning',\n",
       " 'or',\n",
       " 'permitting',\n",
       " 'documents',\n",
       " 'relating',\n",
       " 'to',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'low',\n",
       " 'income',\n",
       " 'homeless',\n",
       " 'encampment',\n",
       " 'in',\n",
       " 'the',\n",
       " 'aurora',\n",
       " 'licton',\n",
       " 'springs',\n",
       " 'urban',\n",
       " 'village',\n",
       " 'according',\n",
       " 'to',\n",
       " 'council',\n",
       " 'bill',\n",
       " 'number',\n",
       " '118310',\n",
       " 'ordinance',\n",
       " 'number',\n",
       " '124747',\n",
       " 'there',\n",
       " 'should',\n",
       " 'be',\n",
       " 'an',\n",
       " 'annual',\n",
       " 'report',\n",
       " 'filed',\n",
       " 'regarding',\n",
       " 'the',\n",
       " 'sanctioned',\n",
       " 'encampments',\n",
       " 'in',\n",
       " 'seattle',\n",
       " 'am',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define some parameters  \n",
    "noisy_pos_tags = ['PROP']\n",
    "min_token_length = 2\n",
    "\n",
    "#Function to check if the token is a noise or not  \n",
    "def isNoise(token):     \n",
    "    is_noise = False\n",
    "    if token.pos_ in noisy_pos_tags:\n",
    "        is_noise = True \n",
    "    elif token.is_stop == True:\n",
    "        is_noise = True\n",
    "    elif len(token.string) <= min_token_length:\n",
    "        is_noise = True\n",
    "    return is_noise \n",
    "\n",
    "def cleanup(token, lower = True):\n",
    "    if lower:\n",
    "       token = token.lower()\n",
    "    return token.strip()\n",
    "\n",
    "cleaned_list = [cleanup(word.string) for word in docs if not isNoise(word)]\n",
    "cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str x in lambda to ensure that everything is processable by spacy\n",
    "#civ['proc'] = civ['request_summary'].apply(lambda x: nlp(str(x)))\n",
    "#listy = civ.proc.tolist()\n",
    "s = ''\n",
    "for row in civ['request_summary']:\n",
    "    s += str(row)\n",
    "    \n",
    "doc = nlp(s)\n",
    "tokenizer = Tokenizer(doc.vocab)\n",
    "\n",
    "#for sentence in enumerate(text.sents):\n",
    "#    print(sentence)\n",
    "#    print('')\n",
    "\n",
    "words = [token.lemma_ for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "\n",
    "# noun tokens that arent stop words or punctuations\n",
    "nouns = [token.lemma_ for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == \"NOUN\"]\n",
    "\n",
    "# five most common tokens\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common(5)\n",
    "\n",
    "# five most common noun tokens\n",
    "noun_freq = Counter(nouns)\n",
    "common_nouns = noun_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-fd8c5def41f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m for doc in nlp.pipe(civ['request_summary'].astype('unicode').values, batch_size=50,\n\u001b[0;32m----> 6\u001b[0;31m                         n_threads=3):\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/spacy/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0moriginal_strings_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mnr_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/spacy/lib/python3.6/site-packages/cytoolz/itertoolz.pyx\u001b[0m in \u001b[0;36mcytoolz.itertoolz.partition_all.__next__ (cytoolz/itertoolz.c:14538)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.precompute_hiddens.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/spacy/lib/python3.6/site-packages/spacy/_ml.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         Yf = self.ops.xp.dot(X,\n\u001b[0;32m--> 149\u001b[0;31m             self.W.reshape((self.nF*self.nO*self.nP, self.nI)).T)\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "\n",
    "for doc in nlp.pipe(civ['request_summary'].astype('unicode').values, batch_size=50,\n",
    "                        n_threads=3):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "\n",
    "civ['tokens'] = tokens\n",
    "civ['lemma'] = lemma\n",
    "civ['pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas for analysis\n",
    "is it possible to predict the department that a request goes to, given the body of a request?\n",
    "https://towardsdatascience.com/machine-learning-for-text-classification-using-spacy-in-python-b276b4051a49\n",
    "\n",
    "is it possible to do LDA or topic modeling, to gauge what people are typically writing about in PRRs?\n",
    "\n",
    "is it possible to discover the frequency of a keyword over time, and plot it?\n",
    "\n",
    "it it possible to collect n-grams, and uncover phrases of relevance?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
